{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk.corpus\n",
    "\n",
    "# Getting the Summary (retrieve top 7 sentences and prints them)\n",
    "import heapq \n",
    "\n",
    "# scrape the data from a site\n",
    "def scrape_data_from_site(site):\n",
    "    article_text = \"\"\n",
    "    if(site):\n",
    "        scraped_data = urllib.request.urlopen(site)\n",
    "        article = scraped_data.read()\n",
    "\n",
    "        parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "        paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "        for p in paragraphs:  \n",
    "            article_text += p.text\n",
    "        \n",
    "    return article_text\n",
    "\n",
    "# Removing Square Brackets and Extra Spaces\n",
    "def remove_square_brackets_and_extra_spaces(article_text): \n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "    return re.sub(r'\\s+', ' ', article_text)\n",
    "\n",
    "# Removing special characters and digits\n",
    "def remove_spl_chars_and_digits(article_text):\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "    return re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "\n",
    "# Find Weighted Frequency of Occurrence\n",
    "def find_weighted_frequency_of_occurence(formatted_article_text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}  \n",
    "    for word in nltk.word_tokenize(formatted_article_text):  \n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    \n",
    "    # To find the weighted frequency, we can simply divide the number of occurances \n",
    "    # of all the words by the frequency of the most occurring word\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "    return word_frequencies\n",
    "\n",
    "# Calculating Sentence Scores\n",
    "def calculate_sentence_scores(sentence_list, word_frequencies):\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "    return sentence_scores\n",
    "\n",
    "def retrieve_top_sentences(sentence_scores, num):\n",
    "    summary_sentences = heapq.nlargest(num, sentence_scores, key=sentence_scores.get)\n",
    "    return ' \\n\\n'.join(summary_sentences) \n",
    "\n",
    "def get_top_summary_from_web_page(url, toplines): \n",
    "    article_text = scrape_data_from_site(url)\n",
    "\n",
    "    article_text = remove_square_brackets_and_extra_spaces(article_text)\n",
    "\n",
    "    formatted_article_text = remove_spl_chars_and_digits(article_text)\n",
    "\n",
    "    # Converting Text To Sentences\n",
    "    sentence_list = sent_tokenize(article_text)\n",
    "\n",
    "    word_frequencies = find_weighted_frequency_of_occurence(formatted_article_text)\n",
    "\n",
    "    sentence_scores = calculate_sentence_scores(sentence_list, word_frequencies)\n",
    "\n",
    "    return retrieve_top_sentences(sentence_scores, toplines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = get_top_summary_from_web_page('https://en.wikipedia.org/wiki/Cloud_computing', 5)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"No module named 'google' found\")\n",
    "\n",
    "# to search\n",
    "query = \"Quantum Computing\"\n",
    "\n",
    "sites=[]\n",
    "for j in search(query, tld=\"com\", num=10, start=0, stop=1, pause=2.0):\n",
    "    summary = get_top_summary_from_web_page(j, 5)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
